import os
import requests
import json
from openai import OpenAI
import numpy as np
import re
from datetime import datetime, timedelta
from data.models import News
import time
from difflib import SequenceMatcher

# API í‚¤ë“¤
NEWS_API_KEY = os.getenv('NEWS_API_KEY')
SERPER_API_KEY = os.getenv('SERPER_API_KEY')
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
client = OpenAI(api_key=OPENAI_API_KEY)


def remove_stopwords(text):
    """ê°„ë‹¨í•œ ë¶ˆìš©ì–´ ì œê±°"""
    stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are',
                 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'will', 'would', 'could', 'should'}
    words = text.lower().split()
    return [word for word in words if word not in stopwords and len(word) > 2]


def check_duplicate_title(new_title):
    """ê°œì„ ëœ ì œëª© ìœ ì‚¬ë„ ì²´í¬ (ë¶ˆìš©ì–´ ì œê±° + Jaccard + Levenshtein)"""
    recent_news = News.objects.filter(
        uploaded_at__gte=datetime.now() - timedelta(days=30)
    )

    # ë¶ˆìš©ì–´ ì œê±°ëœ ë‹¨ì–´ì…‹
    new_words = set(remove_stopwords(new_title))

    for news in recent_news:
        old_words = set(remove_stopwords(news.news_title))

        if new_words and old_words:
            # Jaccard ìœ ì‚¬ë„ (ë¶ˆìš©ì–´ ì œê±° í›„)
            jaccard = len(new_words & old_words) / len(new_words | old_words)

            # Levenshtein ìœ ì‚¬ë„
            levenshtein = SequenceMatcher(None, new_title.lower(), news.news_title.lower()).ratio()

            # ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ ì„ê³„ê°’ ë„˜ìœ¼ë©´ ì¤‘ë³µ
            if jaccard > 0.5 or levenshtein > 0.7:
                print(f"ì œëª© ì¤‘ë³µ ê°ì§€: {news.news_title[:30]}... (J:{jaccard:.2f}, L:{levenshtein:.2f})")
                return True
    return False


def determine_news_nation(title, url="", source=""):
    """ì œëª©, URL, ì†ŒìŠ¤ ê¸°ë°˜ìœ¼ë¡œ ë‰´ìŠ¤ êµ­ê°€ ì¶”ì •"""

    # URL ê¸°ë°˜ íŒë‹¨ (ê°€ì¥ ì •í™•)
    if url:
        url_lower = url.lower()
        if any(domain in url_lower for domain in ['cnn.com', 'nytimes.com', 'wsj.com', 'usatoday.com']):
            return "US"
        elif any(domain in url_lower for domain in ['bbc.com', 'guardian.com', 'independent.co.uk', 'telegraph.co.uk']):
            return "UK"
        elif any(domain in url_lower for domain in ['abc.net.au', 'news.com.au', 'theaustralian.com.au']):
            return "AU"

    # ì†ŒìŠ¤ ê¸°ë°˜ íŒë‹¨
    if source:
        source_lower = source.lower()
        if any(s in source_lower for s in ['cnn', 'nytimes', 'usa', 'american']):
            return "US"
        elif any(s in source_lower for s in ['bbc', 'guardian', 'uk', 'british', 'london']):
            return "UK"
        elif any(s in source_lower for s in ['australia', 'sydney', 'melbourne']):
            return "AU"

    # ì œëª© ë‚´ìš© ê¸°ë°˜ íŒë‹¨ (í‚¤ì›Œë“œ)
    title_lower = title.lower()
    us_keywords = ['washington', 'biden', 'trump', 'congress', 'senate', 'white house', 'pentagon', 'fbi', 'nasa',
                   'wall street']
    uk_keywords = ['london', 'westminster', 'downing street', 'parliament', 'british', 'england', 'scotland', 'wales']
    au_keywords = ['sydney', 'melbourne', 'canberra', 'australian', 'australia']

    us_score = sum(1 for keyword in us_keywords if keyword in title_lower)
    uk_score = sum(1 for keyword in uk_keywords if keyword in title_lower)
    au_score = sum(1 for keyword in au_keywords if keyword in title_lower)

    if us_score > uk_score and us_score > au_score:
        return "US"
    elif uk_score > au_score:
        return "UK"
    elif au_score > 0:
        return "AU"

    # ê¸°ë³¸ê°’ (ë¯¸êµ­ì´ ë‰´ìŠ¤ ë¹„ì¤‘ì´ ë†’ìŒ)
    return "US"


def optimize_embedding(embedding):
    """ì„ë² ë”© ìµœì í™” (float16ìœ¼ë¡œ ì••ì¶•)"""
    if embedding:
        try:
            # numpy arrayë¡œ ë³€í™˜ í›„ float16ìœ¼ë¡œ ì••ì¶•
            embedding_array = np.array(embedding, dtype=np.float16)
            # ë‹¤ì‹œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ
            return embedding_array.astype(float).tolist()
        except Exception as e:
            print(f"ì„ë² ë”© ìµœì í™” ì˜¤ë¥˜: {e}")
            return embedding
    return None

def cosine_similarity(a, b):
    """ì •í™•í•œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
    a = np.array(a)
    b = np.array(b)
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

def check_duplicate_expression(expression):
    """ê°œì„ ëœ ì˜ì–´ í‘œí˜„ ì¤‘ë³µ ì²´í¬ (ì˜ë¯¸ ê¸°ë°˜)"""
    used_expressions = News.objects.filter(
        uploaded_at__gte=datetime.now() - timedelta(days=30)
    ).values_list('eng_expression', flat=True)

    # ì •í™•íˆ ê°™ì€ í‘œí˜„
    if expression.lower() in [expr.lower() for expr in used_expressions]:
        print(f"í‘œí˜„ ì •í™• ì¤‘ë³µ: {expression}")
        return True

    # ì„ë² ë”© ê¸°ë°˜ ì˜ë¯¸ ìœ ì‚¬ë„ ì²´í¬
    try:
        new_embedding = get_embedding(expression)
        if not new_embedding:
            return False

        for used_expr in used_expressions:
            used_embedding = get_embedding(used_expr)
            if used_embedding:
                similarity = cosine_similarity(new_embedding, used_embedding)
                if similarity > 0.85:  # ì˜ë¯¸ê°€ ë§¤ìš° ìœ ì‚¬í•˜ë©´ ì¤‘ë³µ
                    print(f"í‘œí˜„ ì˜ë¯¸ ì¤‘ë³µ: {expression} â‰ˆ {used_expr} (ìœ ì‚¬ë„: {similarity:.2f})")
                    return True
    except Exception as e:
        print(f"í‘œí˜„ ìœ ì‚¬ë„ ì²´í¬ ì˜¤ë¥˜: {e}")

    return False


def get_embedding(text):
    """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± (ì˜¤ë¥˜ ì²˜ë¦¬ ê°œì„ )"""
    try:
        response = client.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        return response.data[0].embedding
    except Exception as e:
        print(f"ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {e}")
        return None


def check_content_similarity(title, content):
    """ê°œì„ ëœ ì„ë² ë”© ê¸°ë°˜ ë‚´ìš© ìœ ì‚¬ë„ ì²´í¬"""
    new_text = f"{title} {content}"
    new_embedding = get_embedding(new_text)

    if not new_embedding:
        return False

    recent_news = News.objects.filter(
        uploaded_at__gte=datetime.now() - timedelta(days=30)
    ).exclude(title_embedding__isnull=True)

    for news in recent_news:
        if news.title_embedding:
            try:
                old_embedding = json.loads(news.title_embedding)
                # ì •í™•í•œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                similarity = cosine_similarity(new_embedding, old_embedding)
                if similarity > 0.8:
                    return True
            except Exception as e:
                print(f"ìœ ì‚¬ë„ ê³„ì‚° ì˜¤ë¥˜: {e}")
                continue

    return False


def get_news_data():
    """ë‰´ìŠ¤ + Reddit ë°ì´í„° ìˆ˜ì§‘"""
    all_candidates = []

    # NewsAPI
    try:
        url = "https://newsapi.org/v2/everything"
        params = {
            'apiKey': NEWS_API_KEY,
            'q': 'breaking OR latest OR news OR update',
            'language': 'en',
            'domains': 'bbc.com,cnn.com,theguardian.com,reuters.com,techcrunch.com,engadget.com,politico.com,wsj.com,nytimes.com,washingtonpost.com,independent.co.uk,abc.net.au',
            'pageSize': 30,
            'from': (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
        }

        response = requests.get(url, params=params)
        articles = response.json().get('articles', [])

        for article in articles:
            if article.get('title') and article.get('description'):
                all_candidates.append({
                    'title': article['title'],
                    'content': article['description'],
                    'source': 'news'
                })
    except Exception as e:
        print(f"NewsAPI ì˜¤ë¥˜: {e}")

    # Reddit
    subreddits = ['worldnews', 'technology', 'entertainment', 'politics']
    for sub in subreddits:
        try:
            url = f"https://www.reddit.com/r/{sub}/hot.json?limit=8"
            response = requests.get(url, headers={'User-Agent': 'NewsBot/1.0'})
            data = response.json()

            for post in data.get('data', {}).get('children', []):
                post_data = post.get('data', {})
                title = post_data.get('title', '')
                if title and len(title) > 20:  # ë„ˆë¬´ ì§§ì€ ì œëª© ì œì™¸
                    all_candidates.append({
                        'title': title,
                        'content': title,
                        'source': 'reddit'
                    })
            time.sleep(1)
        except Exception as e:
            print(f"Reddit ì˜¤ë¥˜ ({sub}): {e}")

    return all_candidates


def fact_check(title):
    """ê°•í™”ëœ Serper íŒ©íŠ¸ì²´í¬ + GPT ë³´ì¡° ê²€ì¦"""
    try:
        url = "https://google.serper.dev/search"
        headers = {'X-API-KEY': SERPER_API_KEY, 'Content-Type': 'application/json'}

        response = requests.post(url, json={'q': title, 'num': 5}, headers=headers)
        results = response.json()

        reliable_sources = ['bbc', 'cnn', 'reuters', 'guardian', 'ap', 'nytimes', 'wsj']
        reliable_count = 0
        title_mentions = 0

        for result in results.get('organic', []):
            link = result.get('link', '').lower()
            snippet = result.get('snippet', '').lower()
            result_title = result.get('title', '').lower()

            # ì‹ ë¢°í•  ë§Œí•œ ì†ŒìŠ¤ ì²´í¬
            if any(source in link for source in reliable_sources):
                reliable_count += 1

                # ì œëª©ì´ë‚˜ ìŠ¤ë‹ˆí«ì— ì£¼ìš” í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ ì²´í¬
                title_words = set(title.lower().split()[:3])  # ì œëª© ì• 3ë‹¨ì–´
                if any(word in snippet or word in result_title for word in title_words):
                    title_mentions += 1

        # ê¸°ë³¸ ì‹ ë¢°ë„: ì‹ ë¢°í•  ë§Œí•œ ì†ŒìŠ¤ 2ê°œ ì´ìƒ + ê´€ë ¨ ì–¸ê¸‰ 1ê°œ ì´ìƒ
        if reliable_count >= 2 and title_mentions >= 1:
            return True

        # GPT ë³´ì¡° íŒ©íŠ¸ì²´í¬ (ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ê²½ìš°ë§Œ)
        if reliable_count < 2:
            gpt_check = gpt_fact_check(title)
            return gpt_check

        return reliable_count >= 1

    except Exception as e:
        print(f"íŒ©íŠ¸ì²´í¬ ì˜¤ë¥˜: {e}")
        return True  # ì˜¤ë¥˜ì‹œ ì¼ë‹¨ í†µê³¼


def gpt_fact_check(title):
    """GPT ë³´ì¡° íŒ©íŠ¸ì²´í¬"""
    try:
        prompt = f"""
ë‹¤ìŒ ë‰´ìŠ¤ ì œëª©ì´ ê±°ì§“ë‰´ìŠ¤ì¼ ê°€ëŠ¥ì„±ì„ í‰ê°€í•´ì£¼ì„¸ìš”:
"{title}"

í‰ê°€ ê¸°ì¤€:
- ìƒì‹ì ìœ¼ë¡œ ë§ì´ ë˜ëŠ”ê°€?
- ê³¼ì¥ì´ë‚˜ ì„ ì •ì  í‘œí˜„ì´ ìˆëŠ”ê°€?
- ê²€ì¦ ê°€ëŠ¥í•œ ì‚¬ì‹¤ì¸ê°€?

ë‹µë³€: PASS (ì‹ ë¢°í• ë§Œí•¨) ë˜ëŠ” FAIL (ì˜ì‹¬ìŠ¤ëŸ¬ì›€)
"""

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=10,
            temperature=0.1
        )

        result = response.choices[0].message.content.strip().upper()
        return "PASS" in result

    except Exception as e:
        print(f"GPT íŒ©íŠ¸ì²´í¬ ì˜¤ë¥˜: {e}")
        return True


def calculate_interest_score(title, content):
    """GPTë¡œ í•œêµ­ì¸ ê´€ì‹¬ë„ í‰ê°€ (ë°±ì—… ë¡œì§ ê°•í™”)"""
    text = f"{title} {content}"

    prompt = f"""
ë‹¤ìŒ ë‰´ìŠ¤ê°€ í•œêµ­ 20-30ëŒ€ì—ê²Œ ì–¼ë§ˆë‚˜ í¥ë¯¸ë¡œìš¸ì§€ 0-100ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.

ë‰´ìŠ¤: {text[:200]}

í‰ê°€ ê¸°ì¤€:
- í•œêµ­ê³¼ì˜ ì—°ê´€ì„±
- 20-30ëŒ€ ê´€ì‹¬ì‚¬ (í…Œí¬, ì—°ì˜ˆ, ê²½ì œ, ì‚¬íšŒì´ìŠˆ)
- í™”ì œì„±/í™”ì œê°€ ë  ë§Œí•œ ì •ë„
- êµìœ¡ì  ê°€ì¹˜

ìˆ«ìë§Œ ë‹µë³€: (ì˜ˆ: 75)
"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=10,
            temperature=0.3
        )

        score_text = response.choices[0].message.content.strip()
        score = int(''.join(filter(str.isdigit, score_text)))
        return min(max(score, 0), 100)

    except Exception as e:
        print(f"ê´€ì‹¬ë„ í‰ê°€ ì˜¤ë¥˜: {e}")
        # ê°•í™”ëœ ë°±ì—… ë¡œì§ (í‚¤ì›Œë“œë³„ ê°€ì¤‘ì¹˜)
        keyword_weights = {
            'technology': 20, 'ai': 25, 'celebrity': 15, 'politics': 10,
            'economy': 15, 'climate': 12, 'korea': 30, 'samsung': 20,
            'netflix': 18, 'tesla': 22, 'apple': 20, 'bitcoin': 25
        }

        text_lower = text.lower()
        backup_score = sum(weight for keyword, weight in keyword_weights.items()
                           if keyword in text_lower)
        return min(backup_score, 80)


def generate_korean_news(title, content, url="", source_name=""):
    """GPTë¡œ í•œêµ­ ë‰´ìŠ¤ ìƒì„± (êµ­ê°€ ìë™ ì¶”ì •)"""
    used_expressions = list(News.objects.filter(
        uploaded_at__gte=datetime.now() - timedelta(days=30)
    ).values_list('eng_expression', flat=True))

    # ë‰´ìŠ¤ êµ­ê°€ ë¯¸ë¦¬ ì¶”ì •
    predicted_nation = determine_news_nation(title, url, source_name)

    prompt = f"""
    ë‹¤ìŒ ì˜ì–´ ë‰´ìŠ¤ë¥¼ í•œêµ­ 20-30ëŒ€ê°€ í¥ë¯¸ë¡œì›Œí•  ë‰´ìŠ¤ë¡œ ì¬ì‘ì„±í•´ì£¼ì„¸ìš”.

    ì œëª©: {title}
    ë‚´ìš©: {content}
    ì˜ˆìƒ êµ­ê°€: {predicted_nation}

    ìš”êµ¬ì‚¬í•­:
    1. ì œëª© 45ì ì´ë‚´, ë‚´ìš© 150ì ì´ë‚´
    2. êµ¬ì²´ì ì¸ ì‚¬ì‹¤ê³¼ ìˆ«ì í¬í•¨ (ëˆ„ê°€, ì–¸ì œ, ì–´ë–»ê²Œ, ì™œ)
    3. ìì—°ìŠ¤ëŸ¬ìš´ 20-30ëŒ€ ë¬¸ì²´ ("~í–ˆì–´ìš”", "~ë„¤ìš”", "~ì£ " ì‚¬ìš©)
    4. "~í•©ë‹ˆë‹¤", "~ì…ë‹ˆë‹¤" ê¸ˆì§€
    5. ì¼ìƒíšŒí™” ì˜ì–´í‘œí˜„ ìš°ì„ 
    6. í•œêµ­ê³¼ì˜ ì—°ê´€ì„±ì€ ìì—°ìŠ¤ëŸ¬ìš´ ê²½ìš°ì—ë§Œ (ì–µì§€ë¡œ ë¼ì›Œë„£ì§€ ë§ ê²ƒ)
    7. ì œëª©ì€ ê°„ë‹¨í•˜ì§€ë§Œ ë‚´ìš©íŒŒì•…ì´ ì‰½ê³ , ê°€ë…ì„± ìˆê²Œ ì‘ì„±
    8. ë²ˆì—­ì²´ ê¸ˆì§€, authenticí•œ í•œêµ­ì–´ë¡œ content ì‘ì„±í•˜ê¸°

    ë¬¸ì²´ ì˜ˆì‹œ:
    âŒ "CEOëŠ” ë°œí‘œí–ˆìŠµë‹ˆë‹¤" 
    âœ… "CEOê°€ ê¹œì§ ë°œí‘œí–ˆì–´ìš”"
    âŒ "ê¸°ìˆ ì´ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤"
    âœ… "ê¸°ìˆ ì´ ì—„ì²­ ë°œì „í•˜ê³  ìˆë„¤ìš”"

    ì‚¬ìš©ê¸ˆì§€ í‘œí˜„: {', '.join(used_expressions[-10:])}
    "ì˜ì–´ í‘œí˜„ì€ 1-3ë‹¨ì–´ë¡œ ê°„ë‹¨í•˜ê²Œ ì‘ì„±í•˜ì‹œì˜¤.
    "ê¸´ êµ¬ë¬¸ì´ë‚˜ ë¬¸ì¥ ê¸ˆì§€"
    ì´ˆë“±í•™ìƒë„ ì•Œë²•í•œ ì‰¬ìš´ ì˜ì–´ í‘œí˜„ì€ ì œê³µí•˜ì§€ ì•ŠëŠ”ë‹¤.

    ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€:
    {{
        "news_title": "í•œêµ­ì–´ ì œëª© (45ì ì´ë‚´)",
        "news_content": "í•œêµ­ì–´ ë‚´ìš© (150ì ì´ë‚´, êµ¬ì²´ì  ì‚¬ì‹¤ í¬í•¨)", 
        "news_nation": "{predicted_nation}",
        "eng_expression": "ì˜ì–´í‘œí˜„",
        "korean_expression": "í•œêµ­ì–´ ëœ»",
        "eng_sentence": "ì˜ì–´ ì˜ˆë¬¸",
        "kor_sentence": "í•œêµ­ì–´ ë²ˆì—­"
    }}
    """

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "í•œêµ­ 20-30ëŒ€ìš© ì˜ì–´í•™ìŠµ ë‰´ìŠ¤ ì‘ì„± ì „ë¬¸ê°€. ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=500,
            temperature=0.7
        )

        content = response.choices[0].message.content.strip()

        # JSON ì¶”ì¶œ (ì •ê·œì‹ ì‚¬ìš©ìœ¼ë¡œ ì‹ ë¢°ì„± ê°•í™”)
        try:
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                json_text = json_match.group()
                korean_news = json.loads(json_text)

                # í•„ìˆ˜ í•„ë“œ ê²€ì¦
                required_fields = ['news_title', 'news_content', 'news_nation', 'eng_expression', 'korean_expression',
                                   'eng_sentence', 'kor_sentence']
                if all(field in korean_news for field in required_fields):
                    # news_nation ê°’ ê²€ì¦ ë° ë³´ì •
                    if korean_news['news_nation'] not in ['US', 'UK', 'AU']:
                        korean_news['news_nation'] = predicted_nation
                        print(f"êµ­ê°€ ë³´ì •: {korean_news['news_nation']} â†’ {predicted_nation}")

                    # ìƒì„±ëœ ë‰´ìŠ¤ ê²€ìˆ˜
                    if quality_check(korean_news):
                        return korean_news
                    else:
                        print("í’ˆì§ˆ ê²€ìˆ˜ ì‹¤íŒ¨")
                        return None
                else:
                    print(f"í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {korean_news}")
                    return None
            else:
                print("JSON í˜•ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return None

        except json.JSONDecodeError as e:
            print(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
            print(f"ì‘ë‹µ ë‚´ìš©: {content[:100]}...")
            return None

    except Exception as e:
        print(f"ë‰´ìŠ¤ ìƒì„± ì˜¤ë¥˜: {e}")
        return None


def quality_check(korean_news):
    """ìƒì„±ëœ ë‰´ìŠ¤ í’ˆì§ˆ ê²€ìˆ˜"""
    try:
        prompt = f"""
ë‹¤ìŒ ë‰´ìŠ¤ì— ì˜¤íƒ€ë‚˜ ì´ìƒí•œ ë¬¸ì¥ì€ ì—†ëŠ”ì§€ ê²€ìˆ˜í•´ì£¼ì„¸ìš”:

ì œëª©: {korean_news['news_title']}
ë‚´ìš©: {korean_news['news_content']}
ì˜ì–´í‘œí˜„: {korean_news['eng_expression']} - {korean_news['korean_expression']}
ì˜ˆë¬¸: {korean_news['eng_sentence']} / {korean_news['kor_sentence']}

ë¬¸ì œì—†ìœ¼ë©´ "PASS", ë¬¸ì œìˆìœ¼ë©´ "FAIL"ë¡œ ë‹µë³€
"""

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=10,
            temperature=0.1
        )

        result = response.choices[0].message.content.strip().upper()
        return "PASS" in result

    except Exception as e:
        print(f"í’ˆì§ˆ ê²€ìˆ˜ ì˜¤ë¥˜: {e}")
        return True  # ì˜¤ë¥˜ì‹œ í†µê³¼


def create_daily_news():
    """ë©”ì¸ ë‰´ìŠ¤ ìƒì„± í•¨ìˆ˜"""
    print("ğŸš€ ë‰´ìŠ¤ ìƒì„± ì‹œì‘")

    # ì˜¤ëŠ˜ ë‰´ìŠ¤ ê°œìˆ˜ ì²´í¬
    today_count = News.objects.filter(
        uploaded_at__date=datetime.now().date()
    ).count()

    if today_count >= 8:
        print("âœ… ì˜¤ëŠ˜ ë‰´ìŠ¤ 8ê°œ ì™„ë£Œ")
        return

    need_count = 8 - today_count
    print(f"ğŸ“ {need_count}ê°œ ë‰´ìŠ¤ í•„ìš”")

    # 1. ë°ì´í„° ìˆ˜ì§‘
    candidates = get_news_data()
    print(f"ğŸ“Š {len(candidates)}ê°œ í›„ë³´ ìˆ˜ì§‘")

    # 2. ê´€ì‹¬ë„ ì ìˆ˜ë¡œ ì •ë ¬
    scored_candidates = []
    for candidate in candidates:
        if not check_duplicate_title(candidate['title']):
            score = calculate_interest_score(candidate['title'], candidate['content'])
            if score >= 20:
                candidate['score'] = score
                scored_candidates.append(candidate)

    scored_candidates.sort(key=lambda x: x['score'], reverse=True)
    print(f"âœ… {len(scored_candidates)}ê°œ í›„ë³´ ì„ ë³„")

    # 3. ë‰´ìŠ¤ ìƒì„±
    created_count = 0

    for candidate in scored_candidates[:need_count * 3]:  # ì—¬ìœ ë¶„
        if created_count >= need_count:
            break

        # íŒ©íŠ¸ì²´í¬
        if not fact_check(candidate['title']):
            print(f"âŒ íŒ©íŠ¸ì²´í¬ ì‹¤íŒ¨: {candidate['title'][:30]}")
            continue

        # ì„ë² ë”© ìœ ì‚¬ë„ ì²´í¬
        if check_content_similarity(candidate['title'], candidate['content']):
            print(f"âŒ ë‚´ìš© ìœ ì‚¬: {candidate['title'][:30]}")
            continue

        # GPT ìƒì„±
        korean_news = generate_korean_news(
            candidate['title'],
            candidate['content'],
            candidate.get('url', ''),
            candidate.get('source_name', '')
        )
        if not korean_news:
            continue

        # ì˜ì–´í‘œí˜„ ì¤‘ë³µ ì²´í¬
        if check_duplicate_expression(korean_news['eng_expression']):
            print(f"âŒ í‘œí˜„ ì¤‘ë³µ: {korean_news['eng_expression']}")
            continue

        # ì„ë² ë”© ìƒì„± ë° ìµœì í™”
        new_text = f"{korean_news['news_title']} {korean_news['news_content']}"
        embedding = get_embedding(new_text)
        optimized_embedding = optimize_embedding(embedding)

        try:
            news_obj = News.objects.create(
                news_title=korean_news['news_title'][:50],
                news_content=korean_news['news_content'][:150],
                news_nation=korean_news['news_nation'],
                eng_expression=korean_news['eng_expression'][:50],
                korean_expression=korean_news['korean_expression'][:50],
                eng_sentence=korean_news['eng_sentence'][:100],
                kor_sentence=korean_news['kor_sentence'][:100],
                title_embedding=json.dumps(optimized_embedding) if optimized_embedding else None
            )

            created_count += 1
            print(f"âœ… ë‰´ìŠ¤ ì €ì¥ ì™„ë£Œ ({created_count}/{need_count}): {korean_news['news_title'][:30]}...")
            print(
                f"   ğŸŒ êµ­ê°€: {korean_news['news_nation']} | ğŸ“ í‘œí˜„: {korean_news['eng_expression']} - {korean_news['korean_expression']}")

            # ì„ë² ë”© ì••ì¶• íš¨ê³¼ ë¡œê¹…
            if embedding and optimized_embedding:
                original_size = len(json.dumps(embedding))
                compressed_size = len(json.dumps(optimized_embedding))
                compression_ratio = (1 - compressed_size / original_size) * 100
                print(f"   ğŸ’¾ ì„ë² ë”© ì••ì¶•: {original_size}â†’{compressed_size} bytes ({compression_ratio:.1f}% ì ˆì•½)")

        except Exception as e:
            print(f"âŒ DB ì €ì¥ ì˜¤ë¥˜: {e}")
            print(f"   ë‰´ìŠ¤ ë°ì´í„°: {korean_news}")

        time.sleep(2)

    print(f"ğŸ‰ ì´ {created_count}ê°œ ë‰´ìŠ¤ ìƒì„± ì™„ë£Œ!")